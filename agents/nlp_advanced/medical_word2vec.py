"""
Medical Word2Vec Module
========================
Impl√©mente Word2Vec (CBOW et Skip-gram) sur dataset m√©dical:
- Entra√Ænement CBOW
- Entra√Ænement Skip-gram
- Similarit√© cosinus
- Analogies m√©dicales
- Hypoth√®se distributionnelle
"""

import os
import sys
import json
from typing import List, Dict, Tuple
import numpy as np

# Ajouter le chemin du projet
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

try:
    from gensim.models import Word2Vec
    from gensim.models.callbacks import CallbackAny2Vec
    GENSIM_AVAILABLE = True
except ImportError:
    GENSIM_AVAILABLE = False
    print("‚ö†Ô∏è  gensim non disponible. Installez avec: pip install gensim")


class MedicalWord2Vec:
    """Entra√Æne Word2Vec sur donn√©es m√©dicales"""
    
    def __init__(self, data_path: str = "data/processed/dataset_processed.json"):
        """
        Initialise le module Word2Vec m√©dical
        
        Args:
            data_path: Chemin vers dataset m√©dical
        """
        print("\nüß¨ Initialisation Medical Word2Vec...")
        
        self.data_path = data_path
        self.corpus = []
        self.cbow_model = None
        self.skipgram_model = None
        
        # Charger et pr√©parer le corpus
        self._load_medical_corpus()
        
        print(f"   ‚úÖ Corpus charg√©: {len(self.corpus)} phrases")
    
    def _load_medical_corpus(self):
        """Charge le corpus m√©dical depuis le dataset"""
        print(f"   üìñ Chargement corpus depuis {self.data_path}...")
        
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Extraire les textes patients
            for case in data:
                if 'patient_text' in case:
                    # Tokenizer le texte
                    text = case['patient_text'].lower()
                    tokens = text.split()
                    
                    if len(tokens) > 2:  # Phrases avec au moins 3 mots
                        self.corpus.append(tokens)
            
            print(f"      ‚úÖ {len(self.corpus)} phrases extraites")
            
            # Statistiques
            total_words = sum(len(sentence) for sentence in self.corpus)
            unique_words = len(set(word for sentence in self.corpus for word in sentence))
            
            print(f"      üìä Mots totaux: {total_words}")
            print(f"      üìä Vocabulaire: {unique_words} mots uniques")
            
        except FileNotFoundError:
            print(f"      ‚ö†Ô∏è  Fichier non trouv√©: {self.data_path}")
            print(f"      üìù Utilisation corpus de d√©monstration...")
            
            # Corpus de d√©mo m√©dical
            demo_corpus = [
                "severe headache with nausea and vomiting",
                "chest pain radiating to left arm",
                "stomach pain with fever and chills",
                "difficulty breathing and chest tightness",
                "severe migraine with visual disturbances",
                "abdominal pain with bloating and gas",
                "sharp chest pain when breathing",
                "chronic headache lasting several days",
                "nausea vomiting and diarrhea",
                "pain in lower back radiating to leg"
            ]
            
            self.corpus = [sentence.split() for sentence in demo_corpus]
    
    # =========================================================================
    # CBOW (Continuous Bag-of-Words)
    # =========================================================================
    
    def train_cbow(self, 
                   vector_size: int = 100,
                   window: int = 5,
                   min_count: int = 1,
                   epochs: int = 10,
                   workers: int = 4) -> Word2Vec:
        """
        Entra√Æne un mod√®le CBOW
        
        CBOW pr√©dit le mot central √† partir du contexte
        Context: [w_{t-k}, ..., w_{t-1}, w_{t+1}, ..., w_{t+k}] ‚Üí Target: w_t
        
        Args:
            vector_size: Dimension des vecteurs
            window: Taille de fen√™tre contextuelle
            min_count: Fr√©quence minimale d'un mot
            epochs: Nombre d'√©poques
            workers: Threads parall√®les
        
        Returns:
            Mod√®le Word2Vec CBOW entra√Æn√©
        """
        if not GENSIM_AVAILABLE:
            print("‚ùå gensim non disponible")
            return None
        
        print(f"\nüéØ Entra√Ænement CBOW:")
        print(f"   ‚Ä¢ Vector size: {vector_size}")
        print(f"   ‚Ä¢ Window: {window}")
        print(f"   ‚Ä¢ Min count: {min_count}")
        print(f"   ‚Ä¢ Epochs: {epochs}")
        
        # sg=0 pour CBOW (sg=1 pour Skip-gram)
        self.cbow_model = Word2Vec(
            sentences=self.corpus,
            vector_size=vector_size,
            window=window,
            min_count=min_count,
            workers=workers,
            sg=0,  # CBOW
            epochs=epochs
        )
        
        vocab_size = len(self.cbow_model.wv)
        print(f"   ‚úÖ CBOW entra√Æn√© ({vocab_size} mots dans vocabulaire)")
        
        return self.cbow_model
    
    # =========================================================================
    # SKIP-GRAM
    # =========================================================================
    
    def train_skipgram(self,
                       vector_size: int = 100,
                       window: int = 5,
                       min_count: int = 1,
                       epochs: int = 10,
                       workers: int = 4) -> Word2Vec:
        """
        Entra√Æne un mod√®le Skip-gram
        
        Skip-gram pr√©dit le contexte √† partir du mot central
        Target: w_t ‚Üí Context: [w_{t-k}, ..., w_{t-1}, w_{t+1}, ..., w_{t+k}]
        
        Args:
            vector_size: Dimension des vecteurs
            window: Taille de fen√™tre contextuelle
            min_count: Fr√©quence minimale d'un mot
            epochs: Nombre d'√©poques
            workers: Threads parall√®les
        
        Returns:
            Mod√®le Word2Vec Skip-gram entra√Æn√©
        """
        if not GENSIM_AVAILABLE:
            print("‚ùå gensim non disponible")
            return None
        
        print(f"\nüéØ Entra√Ænement Skip-gram:")
        print(f"   ‚Ä¢ Vector size: {vector_size}")
        print(f"   ‚Ä¢ Window: {window}")
        print(f"   ‚Ä¢ Min count: {min_count}")
        print(f"   ‚Ä¢ Epochs: {epochs}")
        
        # sg=1 pour Skip-gram
        self.skipgram_model = Word2Vec(
            sentences=self.corpus,
            vector_size=vector_size,
            window=window,
            min_count=min_count,
            workers=workers,
            sg=1,  # Skip-gram
            epochs=epochs
        )
        
        vocab_size = len(self.skipgram_model.wv)
        print(f"   ‚úÖ Skip-gram entra√Æn√© ({vocab_size} mots dans vocabulaire)")
        
        return self.skipgram_model
    
    # =========================================================================
    # SIMILARIT√â COSINUS
    # =========================================================================
    
    def get_similar_words(self, word: str, model_type: str = 'cbow', topn: int = 5) -> List[Tuple[str, float]]:
        """
        Trouve les mots les plus similaires
        
        Utilise similarit√© cosinus: cos(Œ∏) = (A ¬∑ B) / (||A|| ||B||)
        
        Args:
            word: Mot de r√©f√©rence
            model_type: 'cbow' ou 'skipgram'
            topn: Nombre de r√©sultats
        
        Returns:
            Liste de (mot, similarit√©)
        
        Example:
            >>> get_similar_words('headache', 'cbow', 3)
            [('migraine', 0.85), ('pain', 0.78), ('severe', 0.65)]
        """
        model = self.cbow_model if model_type == 'cbow' else self.skipgram_model
        
        if not model:
            print(f"‚ùå Mod√®le {model_type} non entra√Æn√©")
            return []
        
        if word not in model.wv:
            print(f"‚ö†Ô∏è  Mot '{word}' non dans vocabulaire")
            return []
        
        similar = model.wv.most_similar(word, topn=topn)
        
        return similar
    
    def compare_similarity(self, word1: str, word2: str, model_type: str = 'cbow') -> float:
        """
        Calcule la similarit√© entre deux mots
        
        Args:
            word1: Premier mot
            word2: Deuxi√®me mot
            model_type: 'cbow' ou 'skipgram'
        
        Returns:
            Score de similarit√© (0-1)
        """
        model = self.cbow_model if model_type == 'cbow' else self.skipgram_model
        
        if not model:
            return 0.0
        
        if word1 not in model.wv or word2 not in model.wv:
            return 0.0
        
        similarity = model.wv.similarity(word1, word2)
        
        return similarity
    
    def demonstrate_similarity(self, words: List[str], model_type: str = 'cbow'):
        """D√©montre la similarit√© cosinus"""
        print(f"\nüìê SIMILARIT√â COSINUS ({model_type.upper()}):")
        
        for word in words:
            similar = self.get_similar_words(word, model_type, topn=5)
            
            if similar:
                print(f"\n   Mots similaires √† '{word}':")
                for sim_word, score in similar:
                    print(f"      ‚Ä¢ {sim_word}: {score:.3f}")
    
    # =========================================================================
    # ANALOGIES
    # =========================================================================
    
    def solve_analogy(self, 
                      positive: List[str], 
                      negative: List[str],
                      model_type: str = 'cbow',
                      topn: int = 1) -> List[Tuple[str, float]]:
        """
        R√©sout une analogie: A - B + C = ?
        
        Exemple m√©dical:
            headache - head + chest = chest pain
            fever - high + low = hypothermia
        
        Args:
            positive: Mots positifs [C, ...]
            negative: Mots n√©gatifs [A, B, ...]
            model_type: 'cbow' ou 'skipgram'
            topn: Nombre de r√©sultats
        
        Returns:
            Liste de (mot, score)
        
        Example:
            >>> solve_analogy(['chest'], ['headache', 'head'])
            [('pain', 0.75)]
        """
        model = self.cbow_model if model_type == 'cbow' else self.skipgram_model
        
        if not model:
            print(f"‚ùå Mod√®le {model_type} non entra√Æn√©")
            return []
        
        # V√©rifier que tous les mots sont dans le vocabulaire
        all_words = positive + negative
        for word in all_words:
            if word not in model.wv:
                print(f"‚ö†Ô∏è  Mot '{word}' non dans vocabulaire")
                return []
        
        try:
            result = model.wv.most_similar(
                positive=positive,
                negative=negative,
                topn=topn
            )
            return result
        except Exception as e:
            print(f"‚ùå Erreur analogie: {e}")
            return []
    
    def demonstrate_analogies(self, model_type: str = 'cbow'):
        """D√©montre les analogies m√©dicales"""
        print(f"\nüîÑ ANALOGIES M√âDICALES ({model_type.upper()}):")
        
        # Exemples d'analogies m√©dicales
        analogies = [
            ("headache - head + chest", ['chest'], ['headache', 'head']),
            ("pain + severe", ['pain', 'severe'], []),
            ("nausea + vomiting", ['nausea', 'vomiting'], []),
        ]
        
        for description, positive, negative in analogies:
            result = self.solve_analogy(positive, negative, model_type, topn=3)
            
            if result:
                print(f"\n   {description}:")
                for word, score in result:
                    print(f"      ‚Üí {word} ({score:.3f})")
    
    # =========================================================================
    # COMPARAISON CBOW vs SKIP-GRAM
    # =========================================================================
    
    def compare_models(self, test_words: List[str]):
        """Compare CBOW vs Skip-gram"""
        print(f"\n‚öñÔ∏è  COMPARAISON CBOW vs SKIP-GRAM:")
        
        for word in test_words:
            print(f"\n   Mot: '{word}'")
            
            # CBOW
            cbow_similar = self.get_similar_words(word, 'cbow', topn=3)
            print(f"      CBOW: {[w for w, s in cbow_similar]}")
            
            # Skip-gram
            sg_similar = self.get_similar_words(word, 'skipgram', topn=3)
            print(f"      Skip-gram: {[w for w, s in sg_similar]}")


# ==============================================================================
# TEST
# ==============================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("üß™ TEST MEDICAL WORD2VEC")
    print("="*70)
    
    w2v = MedicalWord2Vec()
    
    # Entra√Æner CBOW
    w2v.train_cbow(vector_size=50, window=3, epochs=20)
    
    # Entra√Æner Skip-gram
    w2v.train_skipgram(vector_size=50, window=3, epochs=20)
    
    # Test mots
    test_words = ['pain', 'headache', 'chest', 'severe']
    
    # Similarit√©s
    w2v.demonstrate_similarity(test_words, 'cbow')
    w2v.demonstrate_similarity(test_words, 'skipgram')
    
    # Analogies
    w2v.demonstrate_analogies('cbow')
    w2v.demonstrate_analogies('skipgram')
    
    # Comparaison
    w2v.compare_models(['pain', 'chest'])
    
    print("\n" + "="*70)
    print("‚úÖ TESTS TERMIN√âS")
    print("="*70)